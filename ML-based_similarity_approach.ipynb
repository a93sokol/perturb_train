{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2357a85-4d3f-4793-9b5b-a356c9cf75c0",
   "metadata": {},
   "source": [
    "## Nonlinear DEGs from PerturbSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef338e8d-6c1a-4e0f-95bd-489a295a13da",
   "metadata": {},
   "source": [
    "#### Background\n",
    "- We have got a reactome similarity matrix between the perturbed genes, both in desiase and healthly.\n",
    "- We know that DEGs-based similarity matrix has a statistically significant overlap with the reactome similarty matrix. \n",
    "- We want to extract knowledge from the perturbSeq dataset which would overlap with the reactome pathways.\n",
    "- We already tried to build a VAE-based model learning an interpretable representation of every perturbation. Even though it gives promising results (statistically significant overlap with reactome similarity mtx), it requires further tuning/refinement as the stability of the solution is low. Specifically, adjustment of the loss function and the learning concept might be needed.\n",
    "- At this point we operate on single-gene perturbations only due to their broader availability. We might use 2-gene perturbations for validation of our hypotheses and expansion of the method in the following iterations.\n",
    "\n",
    "#### The Why\n",
    "- Why do we believe it is reasonable to do? DEGs are computed using t-test and have a statistically significant overlap with reactome. If we incorporate nonlinearity into DEGs in some way, we might get an even stronger result.\n",
    "- Why do we want to do it? It would give us a subset of perspective relationships between genes whose exploration would potentially lead to new biological findings.\n",
    "- **Optional addon:** We know that statistical methods used for DEGs do not use feature interactions (they might but it is uncommon). We could introduce a model which would focus on feature interactions only. This way we could have effect from the single-feature effects and interactions separately. \n",
    "\n",
    "#### The How\n",
    "- What is the simplest way? Take a binary boosting trees classifier (lgbm or catboost), and find the top useful features for every perturbation, for the model classifying into ctrl vs perturbed state. Ofc, one can play with SHAPs, etc. \n",
    "- What is a more advanced approach? Having a similarity matrix from the previous step, we can refine it by building binary classfiers for perturbation A vs perturbation B task. This would allow to refine differences between the perturbations. The classifier performance and its feature importances (or SHAPs) would be used for the refinement.\n",
    "- Check the result consistency with HPT and swapping the classifier.\n",
    "\n",
    "#### Performance of the approach\n",
    "- How to assess the success of the approach? Hmm...\n",
    "  - Check the overlap with the DEGs - for exploration purposes only, does not allow to assess usability\n",
    "  - Consider TF pairs which were not explored together and are similar according to our approach. Play with them in the wetlab to get a proof that they are connected? Is it of any use at all?\n",
    "  \n",
    "#### Thoughts and feedback\n",
    "- Network analysis: Construct gene interaction networks based on your results and compare their properties (e.g., modularity, centrality) with known biological networks.\n",
    "- Consider using multi-class classification instead of multiple binary classifiers to potentially capture global patterns.\n",
    "- Implement a hierarchical classification scheme to group similar perturbations - this is basically a sequence of splits by multiple models following the known hierarchy. We have got the hierarchy from the VAE-based model or we can get one from the hierarchical clustering. Now, we can refine the paths. Here are several approaches we can consider for that:\n",
    "    - Start with your initial hierarchy.\n",
    "    - Train classifiers at each node and evaluate their performance.\n",
    "    - Identify nodes where performance is poor.\n",
    "    - For these problematic nodes, consider:\n",
    "        - a) Splitting the node further if it's too heterogeneous.\n",
    "        - b) Merging with a sibling node if they're too similar.\n",
    "        - c) Adjusting the perturbations within the node - check the ones contributing to the error the most and change their class.\n",
    "    - Repeat this process iteratively.\n",
    "- **Reasons for only around 10% overlap between DEGs and the pathways?**\n",
    "  - a) The genes are regulated by the same TF/protein/signalling mechanism but have nothing else in common (called indirect relationships).\n",
    "    - This can still be reported in pathways DBs, e.g. WikiPathways, right? \n",
    "  - b) Incomplete pathway annotations - they might be actually related but we do not know that yet.\n",
    "    - This is our research gap\n",
    "  - c) Spurious correlations, cell-type or condition-specific, technical artifacts, genomic proximity, temporal nature of the coexpression.\n",
    "    - We account for it by analysing multiple datasets with different cell types, timelines, conditions, etc. This should address most of these issues.  \n",
    "    \n",
    "#### To decide on the path, we need to see what brings us to the unknown biology faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf26e58-960e-40d1-b160-be4876747e6f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7da009-644e-4c15-a074-ab1671daf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as scp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28545cb6-f504-4523-8ea6-0e37ec165a9f",
   "metadata": {},
   "source": [
    "### Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2d1aad38-4497-4f8c-b5ed-c17b65b5fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_PER_CELL_BINNING = True\n",
    "N_BINS = 1000\n",
    "N_ITER = 50\n",
    "TOP_N_GENES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d6d85-c909-43be-9845-e80618492b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59212c8-94ed-405f-b278-5d8c8440a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as scp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import ot\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789bed1-19f7-46bd-9bab-bfdfbf25afb2",
   "metadata": {},
   "source": [
    "### Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950ca013-f2b9-42ae-82a7-189c1d06b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_PER_CELL_BINNING = False\n",
    "N_BINS = 1000\n",
    "N_ITER = 50\n",
    "TOP_N_GENES = 576"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9f6a3-389e-4c51-9171-7388946c0f7a",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0736ebb-bdbb-4add-894a-9eff42db19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scp.read_h5ad('./data/Norman_2019/norman_umi_go/perturb_processed.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91f0ae14-2d99-4647-abac-dc157ba9bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs = pd.read_csv('little_data/TF_db.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa427b0-e496-40cd-a40e-cef59fcfbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:, adata.var.loc[adata.var.index.isin(TFs['Ensembl ID'].values)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a33a08a-bd5a-4853-b1fb-c34cd464abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "## Following the scGPT paper, we bin the genes within cell. \n",
    "\n",
    "def bin_nonzero_values(arr, num_bins):\n",
    "    # Filter out non-zero values\n",
    "    nonzero_vals = arr[arr != 0]\n",
    "    \n",
    "    # Calculate bin edges\n",
    "    bin_edges = np.linspace(nonzero_vals.min(), nonzero_vals.max(), num_bins)\n",
    "    \n",
    "    # Bin the values\n",
    "    binned_values = np.zeros_like(arr)\n",
    "    binned_nonzero = np.digitize(nonzero_vals, bin_edges)\n",
    "    binned_values[arr != 0] = binned_nonzero\n",
    "    \n",
    "    return binned_values\n",
    "\n",
    "# Example usage\n",
    "arr = np.random.randint(low=0, high=100, size=100)\n",
    "num_bins = 3\n",
    "binned_values = bin_nonzero_values(arr, num_bins)\n",
    "print(set(binned_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d4184ac-da02-4524-a2c3-1aea429e1047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: adata.X seems to be already log-transformed.\n"
     ]
    }
   ],
   "source": [
    "scp.pp.normalize_total(adata, exclude_highly_expressed=True)\n",
    "scp.pp.log1p(adata)\n",
    "scp.pp.highly_variable_genes(adata, n_top_genes=TOP_N_GENES,subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710331dd-1237-4e68-9683-50bcf14d47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENE_PER_CELL_BINNING:\n",
    "    tempy = adata.X.toarray()\n",
    "    \n",
    "    for c in tqdm(range(adata.X.shape[0])):\n",
    "        tempy[c,:] = bin_nonzero_values(tempy[c,:], N_BINS)\n",
    "    \n",
    "    adata.X = sparse.csr_matrix(tempy)\n",
    "    del tempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f834c24b-f867-4b0f-a6f8-aa9d378127fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adata.obs.condition.values.astype(str)\n",
    "X = adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ee417a-ef48-427b-b316-7884a456d5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91205/91205 [00:00<00:00, 675115.72it/s]\n"
     ]
    }
   ],
   "source": [
    "gene_num_map = ['ctrl']\n",
    "\n",
    "for rec in tqdm(y):\n",
    "    comps = rec.split('+')\n",
    "    for c in comps:\n",
    "        if c not in gene_num_map:\n",
    "            gene_num_map.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "774df632-adda-4147-9219-3ea83dd405fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [06:50<00:00,  3.91s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Assuming you have X (features) and y (labels) already defined\n",
    "# Replace with your actual data\n",
    "\n",
    "# Create a dictionary to store top N important features for each dataset\n",
    "top_features_dict = {}\n",
    "feats = adata.var.gene_name.values\n",
    "# Define the number of top features to select\n",
    "N = 50\n",
    "\n",
    "# Iterate over each gene in gene_num_map\n",
    "for g in tqdm(gene_num_map[1:]):\n",
    "    # Create a mask for the current gene\n",
    "    mask = np.array([(g in element and 'ctrl' in element) or element == 'ctrl' for element in y]).astype(bool)\n",
    "    \n",
    "    # Filter the data for the current gene\n",
    "    X_temp = X[mask]\n",
    "    y_temp = y[mask]\n",
    "    y_temp[y_temp != 'ctrl'] = 1\n",
    "    y_temp[y_temp == 'ctrl'] = 0\n",
    "    \n",
    "    # Split the data into train, validation, and test sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Initialize the CatBoost classifier\n",
    "    clf = CatBoostClassifier(loss_function='Logloss') #iterations=300, depth=7, learning_rate=0.1, \n",
    "    \n",
    "    # Fit the classifier on the training data\n",
    "    clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10, verbose=0)\n",
    "    \n",
    "    # Get feature importances\n",
    "    feature_importances = clf.feature_importances_\n",
    "    \n",
    "    # Get indices of top N important features\n",
    "    top_feature_indices = np.argsort(feature_importances)[-N:]\n",
    "    \n",
    "    # Get the actual feature names\n",
    "    top_features = feats[top_feature_indices]\n",
    "    \n",
    "    # Store the top features in the dictionary\n",
    "    top_features_dict[g] = top_features\n",
    "\n",
    "# Create a pandas DataFrame from the dictionary\n",
    "top_features_df = pd.DataFrame.from_dict(top_features_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f5877686-a5e0-4ddf-a206-52a324da8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features_df.to_csv('./little_data/CatBoost_top50_important_for_binary_classify.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "77cc7bb4-3c18-4a26-8012-c5ff224e748d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [00:27<00:00,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sim_mtx = pd.DataFrame(index = top_features_df.index, columns = top_features_df.index)\n",
    "for g1 in tqdm(top_features_df.index):\n",
    "    for g2 in top_features_df.index:\n",
    "        \n",
    "        t1 = top_features_df.loc[g1].values.tolist()\n",
    "        t2 = top_features_df.loc[g2].values.tolist()\n",
    "\n",
    "        m = top_features_df.loc[g1].isin(t2)\n",
    "        d1 = m.loc[m==True].index.values\n",
    "\n",
    "        m = top_features_df.loc[g2].isin(t1)\n",
    "        d2 = m.loc[m==True].index.values\n",
    "        \n",
    "        sim_mtx.loc[g1,g2] = float(len(set(t1).intersection(set(t2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ac3f1892-e9a3-4238-9f82-09dc46a97739",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mtx.to_csv('./little_data/CatBoost_sim_mtx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a0a4d6-f7e7-4006-880b-ca3deb29e37d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
