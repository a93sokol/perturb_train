{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2357a85-4d3f-4793-9b5b-a356c9cf75c0",
   "metadata": {},
   "source": [
    "## Nonlinear DEGs from PerturbSeq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef338e8d-6c1a-4e0f-95bd-489a295a13da",
   "metadata": {},
   "source": [
    "#### Background\n",
    "- We have got a reactome similarity matrix between the perturbed genes, both in desiase and healthly.\n",
    "- We know that DEGs-based similarity matrix has a statistically significant overlap with the reactome similarty matrix. \n",
    "- We want to extract knowledge from the perturbSeq dataset which would overlap with the reactome pathways.\n",
    "- We already tried to build a VAE-based model learning an interpretable representation of every perturbation. Even though it gives promising results (statistically significant overlap with reactome similarity mtx), it requires further tuning/refinement as the stability of the solution is low. Specifically, adjustment of the loss function and the learning concept might be needed.\n",
    "- At this point we operate on single-gene perturbations only due to their broader availability. We might use 2-gene perturbations for validation of our hypotheses and expansion of the method in the following iterations.\n",
    "\n",
    "#### The Why\n",
    "- Why do we believe it is reasonable to do? DEGs are computed using t-test and have a statistically significant overlap with reactome. If we incorporate nonlinearity into DEGs in some way, we might get an even stronger result.\n",
    "- Why do we want to do it? It would give us a subset of perspective relationships between genes whose exploration would potentially lead to new biological findings.\n",
    "- **Optional addon:** We know that statistical methods used for DEGs do not use feature interactions (they might but it is uncommon). We could introduce a model which would focus on feature interactions only. This way we could have effect from the single-feature effects and interactions separately. \n",
    "\n",
    "#### The How\n",
    "- What is the simplest way? Take a binary boosting trees classifier (lgbm or catboost), and find the top important features for every perturbation, for the model classifying into ctrl vs perturbed state. Ofc, one can play with SHAPs, etc. \n",
    "- What is a more advanced approach? Having a similarity matrix from the previous step, we can refine it by building binary classfiers for perturbation A vs perturbation B task. This would allow to refine differences between the perturbations. The classifier performance and its feature importances (or SHAPs) would be used for the refinement.\n",
    "- Check the result consistency with HPT and swapping the classifier.\n",
    "\n",
    "#### Performance of the approach\n",
    "- How to assess the success of the approach? Hmm...\n",
    "  - Check the overlap with the DEGs - for exploration purposes only, does not allow to assess usability\n",
    "  - Consider TF pairs which were not explored together and are similar according to our approach. Play with them in the wetlab to get a proof that they are connected? Is it of any use at all?\n",
    "  \n",
    "#### Thoughts and feedback\n",
    "- Network analysis: Construct gene interaction networks based on your results and compare their properties (e.g., modularity, centrality) with known biological networks.\n",
    "- Consider using multi-class classification instead of multiple binary classifiers to potentially capture global patterns.\n",
    "- Implement a hierarchical classification scheme to group similar perturbations - this is basically a sequence of splits by multiple models following the known hierarchy. We have got the hierarchy from the VAE-based model or we can get one from the hierarchical clustering. Now, we can refine the paths. Here are several approaches we can consider for that:\n",
    "    - Start with your initial hierarchy.\n",
    "    - Train classifiers at each node and evaluate their performance.\n",
    "    - Identify nodes where performance is poor.\n",
    "    - For these problematic nodes, consider:\n",
    "        - a) Splitting the node further if it's too heterogeneous.\n",
    "        - b) Merging with a sibling node if they're too similar.\n",
    "        - c) Adjusting the perturbations within the node - check the ones contributing to the error the most and change their class.\n",
    "    - Repeat this process iteratively.\n",
    "</br>   </br>   \n",
    "- **Reasons for only around 10% overlap between DEGs and the pathways?**\n",
    "  - a) The genes are regulated by the same TF/protein/signalling mechanism but have nothing else in common (called indirect relationships).\n",
    "    - This can still be reported in pathways DBs, e.g. WikiPathways, right? \n",
    "  - b) Incomplete pathway annotations - they might be actually related but we do not know that yet.\n",
    "    - This is our research gap\n",
    "  - c) Spurious correlations, cell-type or condition-specific, technical artifacts, genomic proximity, temporal nature of the coexpression.\n",
    "    - We account for it by analysing multiple datasets with different cell types, timelines, conditions, etc. This should address most of these issues.  \n",
    "    \n",
    "#### To decide on the path, we need to see what brings us to the unknown biology faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf26e58-960e-40d1-b160-be4876747e6f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7da009-644e-4c15-a074-ab1671daf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as scp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28545cb6-f504-4523-8ea6-0e37ec165a9f",
   "metadata": {},
   "source": [
    "### Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59212c8-94ed-405f-b278-5d8c8440a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as scp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import catboost as cb\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from collections import Counter\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import ot\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import clear_output\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b789bed1-19f7-46bd-9bab-bfdfbf25afb2",
   "metadata": {},
   "source": [
    "### Magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "950ca013-f2b9-42ae-82a7-189c1d06b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENE_PER_CELL_BINNING = False\n",
    "N_BINS = 1000\n",
    "N_ITER = 50\n",
    "TOP_N_GENES = 576"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f9f6a3-389e-4c51-9171-7388946c0f7a",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0736ebb-bdbb-4add-894a-9eff42db19e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = scp.read_h5ad('./data/Norman_2019/norman_umi_go/perturb_processed.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f0ae14-2d99-4647-abac-dc157ba9bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFs = pd.read_csv('little_data/TF_db.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfa427b0-e496-40cd-a40e-cef59fcfbdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = adata[:, adata.var.loc[adata.var.index.isin(TFs['Ensembl ID'].values)].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a33a08a-bd5a-4853-b1fb-c34cd464abad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "## Following the scGPT paper, we bin the genes within cell. \n",
    "\n",
    "def bin_nonzero_values(arr, num_bins):\n",
    "    # Filter out non-zero values\n",
    "    nonzero_vals = arr[arr != 0]\n",
    "    \n",
    "    # Calculate bin edges\n",
    "    bin_edges = np.linspace(nonzero_vals.min(), nonzero_vals.max(), num_bins)\n",
    "    \n",
    "    # Bin the values\n",
    "    binned_values = np.zeros_like(arr)\n",
    "    binned_nonzero = np.digitize(nonzero_vals, bin_edges)\n",
    "    binned_values[arr != 0] = binned_nonzero\n",
    "    \n",
    "    return binned_values\n",
    "\n",
    "# Example usage\n",
    "arr = np.random.randint(low=0, high=100, size=100)\n",
    "num_bins = 3\n",
    "binned_values = bin_nonzero_values(arr, num_bins)\n",
    "print(set(binned_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4184ac-da02-4524-a2c3-1aea429e1047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpcfs/users/a1234104/miniconda3/envs/py310torch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:206: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    }
   ],
   "source": [
    "scp.pp.normalize_total(adata, exclude_highly_expressed=True)\n",
    "scp.pp.log1p(adata)\n",
    "scp.pp.highly_variable_genes(adata, n_top_genes=TOP_N_GENES,subset=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710331dd-1237-4e68-9683-50bcf14d47d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if GENE_PER_CELL_BINNING:\n",
    "    tempy = adata.X.toarray()\n",
    "    \n",
    "    for c in tqdm(range(adata.X.shape[0])):\n",
    "        tempy[c,:] = bin_nonzero_values(tempy[c,:], N_BINS)\n",
    "    \n",
    "    adata.X = sparse.csr_matrix(tempy)\n",
    "    del tempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f834c24b-f867-4b0f-a6f8-aa9d378127fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = adata.obs.condition.values.astype(str)\n",
    "X = adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09ee417a-ef48-427b-b316-7884a456d5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91205/91205 [00:00<00:00, 894821.35it/s]\n"
     ]
    }
   ],
   "source": [
    "gene_num_map = ['ctrl']\n",
    "\n",
    "for rec in tqdm(y):\n",
    "    comps = rec.split('+')\n",
    "    for c in comps:\n",
    "        if c not in gene_num_map:\n",
    "            gene_num_map.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c3b2da-78bd-4851-bd20-59ca41297484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #################### Feature Importance Approach #####################\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# # Assuming you have X (features) and y (labels) already defined\n",
    "# # Replace with your actual data\n",
    "\n",
    "# # Create a dictionary to store top N important features for each dataset\n",
    "# top_features_dict = {}\n",
    "# feats = adata.var.gene_name.values\n",
    "# # Define the number of top features to select\n",
    "# N = 20\n",
    "\n",
    "# # Iterate over each gene in gene_num_map\n",
    "# for g in tqdm(gene_num_map[1:]):\n",
    "#     # Create a mask for the current gene\n",
    "#     mask = np.array([(g in element and 'ctrl' in element) or element == 'ctrl' for element in y]).astype(bool)\n",
    "    \n",
    "#     # Filter the data for the current gene\n",
    "#     X_temp = X[mask]\n",
    "#     y_temp = y[mask]\n",
    "#     y_temp[y_temp != 'ctrl'] = 1\n",
    "#     y_temp[y_temp == 'ctrl'] = 0\n",
    "    \n",
    "#     # Split the data into train, validation, and test sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
    "#     #X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)\n",
    "    \n",
    "#     # Initialize the CatBoost classifier\n",
    "#     #clf = CatBoostClassifier(loss_function='Logloss') #iterations=300, depth=7, learning_rate=0.1, \n",
    "#     clf = CatBoostClassifier(\n",
    "#                                 iterations=1000,\n",
    "#                                 learning_rate=0.05,\n",
    "#                                 depth=6,\n",
    "#                                 l2_leaf_reg=3,\n",
    "#                                 random_strength=1,\n",
    "#                                 bagging_temperature=1,\n",
    "#                                 od_type='Iter',\n",
    "#                                 od_wait=50,\n",
    "#                                 verbose=0,\n",
    "#                                 auto_class_weights='Balanced'\n",
    "#                             )\n",
    "    \n",
    "#     # Fit the classifier on the training data\n",
    "#     clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10, verbose=0)\n",
    "    \n",
    "#     # Get feature importances\n",
    "#     feature_importances = clf.feature_importances_\n",
    "    \n",
    "#     # Get indices of top N important features\n",
    "#     top_feature_indices = np.argsort(feature_importances)[-N:]\n",
    "    \n",
    "#     # Get the actual feature names\n",
    "#     top_features = feats[top_feature_indices]\n",
    "    \n",
    "#     # Store the top features in the dictionary\n",
    "#     top_features_dict[g] = top_features\n",
    "\n",
    "# # Create a pandas DataFrame from the dictionary\n",
    "# top_features_df = pd.DataFrame.from_dict(top_features_dict).T\n",
    "\n",
    "# # import shap\n",
    "\n",
    "# # # Assuming you have X (features) and y (labels) already defined\n",
    "# # # Create a dictionary to store top N important features for each dataset\n",
    "# # top_features_dict = {}\n",
    "# # feats = adata.var.gene_name.values\n",
    "\n",
    "# # # Define the number of top features to select\n",
    "# # N = 50\n",
    "\n",
    "# # # Iterate over each gene in gene_num_map\n",
    "# # for g in tqdm(gene_num_map[1:]):\n",
    "# #     # Create a mask for the current gene\n",
    "# #     mask = np.array([(g in element and 'ctrl' in element) or element == 'ctrl' for element in y]).astype(bool)\n",
    "    \n",
    "# #     # Filter the data for the current gene\n",
    "# #     X_temp = X[mask]\n",
    "# #     y_temp = y[mask]\n",
    "# #     y_temp = np.where(y_temp == 'ctrl', 0, 1)  # 'ctrl' as 0, 'perturbation' as 1\n",
    "    \n",
    "# #     # Split the data into train and test sets\n",
    "# #     X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    \n",
    "# #     # Initialize the CatBoost classifier\n",
    "# #     #clf = CatBoostClassifier(loss_function='Logloss', verbose=0,  auto_class_weights='Balanced')\n",
    "# #     clf = CatBoostClassifier(\n",
    "# #                                 iterations=2000,\n",
    "# #                                 learning_rate=0.02,\n",
    "# #                                 depth=10,\n",
    "# #                                 l2_leaf_reg=1,\n",
    "# #                                 random_strength=0.5,\n",
    "# #                                 rsm=0.8,\n",
    "# #                                 one_hot_max_size=10,\n",
    "# #                                 leaf_estimation_iterations=10,\n",
    "# #                                 max_ctr_complexity=3,\n",
    "# #                                 verbose=0,\n",
    "# #                                 auto_class_weights='Balanced'\n",
    "# #                             )\n",
    "    \n",
    "# #     # Fit the classifier on the training data\n",
    "# #     clf.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=10, verbose=0)\n",
    "    \n",
    "# #     # Make predictions on the test set\n",
    "# #     y_pred = clf.predict(X_test)\n",
    "    \n",
    "# #     # Get indices of correctly classified samples\n",
    "# #     correct_indices = np.where(y_pred == y_test)[0]\n",
    "    \n",
    "# #     # Filter X_test for correctly classified samples\n",
    "# #     X_test_correct = X_test[correct_indices]\n",
    "# #     y_test_correct = y_test[correct_indices]\n",
    "    \n",
    "# #     # Get control samples to use as reference\n",
    "# #     X_ctrl = X_test_correct[y_test_correct == 0]\n",
    "    \n",
    "# #     # Initialize the SHAP explainer with control samples as reference\n",
    "# #     explainer = shap.TreeExplainer(clf, data=X_ctrl, feature_perturbation = 'interventional')\n",
    "    \n",
    "# #     # Calculate SHAP values for correctly classified perturbation samples\n",
    "    \n",
    "# #     X_pert = X_test_correct[y_test_correct == 1]\n",
    "# #     y_pert = y_test_correct[y_test_correct == 1]\n",
    "    \n",
    "# #     if X_pert.shape[0]<1:\n",
    "# #         print(f'{g} has no correctly classified perturbed records')\n",
    "# #         continue\n",
    "    \n",
    "# #     shap_values = explainer.shap_values(X_pert)\n",
    "    \n",
    "# #     # SHAP values are returned as a list of arrays, one for each class\n",
    "# #     # We're interested in the SHAP values for the 'perturbation' class (index 1)\n",
    "# #     shap_values_perturbation = shap_values[y_pert==1]\n",
    "    \n",
    "# #     # Calculate mean absolute SHAP values across samples\n",
    "# #     mean_shap_values = np.median(np.abs(shap_values_perturbation), axis=0)\n",
    "    \n",
    "# #     # Calculate the maximum SHAP value\n",
    "# #     max_shap_value = np.max(mean_shap_values)\n",
    "    \n",
    "# #     # Get indices of features with impact > 10% of max impact\n",
    "# #     significant_feature_indices = np.where(mean_shap_values > 0.35 * max_shap_value)[0]\n",
    "    \n",
    "# #     # Sort these indices by SHAP value (descending order)\n",
    "# #     sorted_significant_indices = significant_feature_indices[np.argsort(-mean_shap_values[significant_feature_indices])]\n",
    "    \n",
    "# #     # Get the actual feature names (up to N features)\n",
    "# #     top_features = feats[sorted_significant_indices] #[:N]\n",
    "    \n",
    "# #     # Store the top features in the dictionary\n",
    "# #     top_features_dict[g] = top_features\n",
    "\n",
    "# # # Create a pandas DataFrame from the dictionary\n",
    "# # #top_features_df = pd.DataFrame.from_dict(top_features_dict).T\n",
    "\n",
    "# sim_mtx = pd.DataFrame(index = top_features_df.index, columns = top_features_df.index)\n",
    "# for g1 in tqdm(top_features_df.index):\n",
    "#     for g2 in top_features_df.index:\n",
    "        \n",
    "#         t1 = top_features_dict[g1].tolist()\n",
    "#         t2 = top_features_dict[g2].tolist()\n",
    "\n",
    "#         # m = top_features_df.loc[g1].isin(t2)\n",
    "#         # d1 = m.loc[m==True].index.values\n",
    "\n",
    "#         # m = top_features_df.loc[g2].isin(t1)\n",
    "#         # d2 = m.loc[m==True].index.values\n",
    "        \n",
    "#         sim_mtx.loc[g1,g2] = float(len(set(t1).intersection(set(t2))))\n",
    "\n",
    "\n",
    "# sim_mtx.to_csv('./little_data/CatBoost_sim_mtx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4a97ae18-be82-43f0-81c3-f6ca1d6f2a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assuming you have X (features), y (labels), and adata already defined\n",
    "\n",
    "def get_top_features(X, y, feats, N=20):\n",
    "    top_features_dict = {}\n",
    "    \n",
    "    clf = CatBoostClassifier(\n",
    "        iterations=1000,\n",
    "        learning_rate=0.05,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=3,\n",
    "        random_strength=1,\n",
    "        bagging_temperature=1,\n",
    "        od_type='Iter',\n",
    "        od_wait=50,\n",
    "        verbose=0,\n",
    "        auto_class_weights='Balanced'\n",
    "    )\n",
    "\n",
    "    for g in tqdm(gene_num_map[1:]):\n",
    "        mask = np.array([(g in element and 'ctrl' in element) or element == 'ctrl' for element in y])\n",
    "        X_temp, y_temp = X[mask], y[mask]\n",
    "        y_temp = np.where(y_temp == 'ctrl', 0, 1)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15, random_state=42)\n",
    "        \n",
    "        clf.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=10, verbose=0)\n",
    "        \n",
    "        top_feature_indices = np.argsort(clf.feature_importances_)[-N:]\n",
    "        top_features_dict[g] = feats[top_feature_indices]\n",
    "    \n",
    "    return pd.DataFrame.from_dict(top_features_dict).T\n",
    "\n",
    "def compute_similarity_matrix(top_features_df):\n",
    "    sim_mtx = pd.DataFrame(index=top_features_df.index, columns=top_features_df.index)\n",
    "    for g1 in tqdm(top_features_df.index):\n",
    "        for g2 in top_features_df.index:\n",
    "            sim_mtx.loc[g1, g2] = len(set(top_features_df.loc[g1]) & set(top_features_df.loc[g2]))\n",
    "    return sim_mtx\n",
    "\n",
    "def build_binary_classifiers(X, y, sim_mtx, quantile=0.95):\n",
    "    performance_mtx = pd.DataFrame(index=sim_mtx.index, columns=sim_mtx.index)\n",
    "    clf = CatBoostClassifier(iterations=500, learning_rate=0.05, depth=6, verbose=0)\n",
    "    threshold = np.quantile(sim_mtx.values.ravel(), quantile)\n",
    "    \n",
    "    for g1 in tqdm(sim_mtx.index):  \n",
    "        # Find similar genes based on the quantile threshold\n",
    "        similar_genes = sim_mtx.index[sim_mtx[g1] >= threshold]\n",
    "        \n",
    "        for g2 in similar_genes:\n",
    "            if g1 != g2:\n",
    "                mask = np.array([g1 in str(element) or g2 in str(element) for element in y])\n",
    "                X_temp, y_temp = X[mask], y[mask]\n",
    "                y_temp = np.array([1 if g1 in str(label) else 0 for label in y_temp])\n",
    "\n",
    "                # Only proceed if we have enough samples of each class\n",
    "                if np.min(np.bincount(y_temp)) >= 20:  # Ensure at least 10 samples in each class\n",
    "                    X_train, X_test, y_train, y_test = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42, stratify=y_temp)\n",
    "                    \n",
    "                    clf.fit(X_train, y_train, verbose=False)\n",
    "                    y_pred = clf.predict_proba(X_test)[:, 1]\n",
    "                    \n",
    "                    try:\n",
    "                        performance_mtx.loc[g1, g2] = roc_auc_score(y_test, y_pred)\n",
    "                    except ValueError:\n",
    "                        # This can happen if y_test contains only one class\n",
    "                        performance_mtx.loc[g1, g2] = np.nan\n",
    "\n",
    "    return performance_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1202a536-2ff9-459f-bb9e-a8571917ebaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "feats = adata.var.gene_name.values\n",
    "top_features_df = get_top_features(X, y, feats)\n",
    "sim_mtx = compute_similarity_matrix(top_features_df)\n",
    "sim_mtx.to_csv('./little_data/CatBoost_sim_mtx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "caba4683-85d9-42ce-994b-c1a40cc7ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mtx = pd.read_csv('./little_data/CatBoost_sim_mtx.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "729fa07a-3b34-415e-a2df-5b113268a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 105/105 [1:07:26<00:00, 38.54s/it]\n"
     ]
    }
   ],
   "source": [
    "performance_mtx = build_binary_classifiers(X, y, sim_mtx, quantile = 0.95)\n",
    "performance_mtx.to_csv('./little_data/CatBoost_performance_mtx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "76f91a6f-5d20-4b5a-8985-6be2612c71e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tmp_local/lls_2376517/ipykernel_3822717/1942265003.py:1: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  performance_mtx.fillna(0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSC22D1</th>\n",
       "      <th>KLF1</th>\n",
       "      <th>MAP2K6</th>\n",
       "      <th>CEBPE</th>\n",
       "      <th>RUNX1T1</th>\n",
       "      <th>MAML2</th>\n",
       "      <th>CBL</th>\n",
       "      <th>PTPN9</th>\n",
       "      <th>TGFBR2</th>\n",
       "      <th>ETS2</th>\n",
       "      <th>...</th>\n",
       "      <th>ATL1</th>\n",
       "      <th>NIT1</th>\n",
       "      <th>CDKN1B</th>\n",
       "      <th>PTPN13</th>\n",
       "      <th>HOXA13</th>\n",
       "      <th>CITED1</th>\n",
       "      <th>PRDM1</th>\n",
       "      <th>HK2</th>\n",
       "      <th>CDKN1C</th>\n",
       "      <th>EGR1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TSC22D1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KLF1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.966037</td>\n",
       "      <td>0.966848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.969909</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAP2K6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893898</td>\n",
       "      <td>0.953321</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEBPE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RUNX1T1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITED1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.897639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRDM1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.983938</td>\n",
       "      <td>0.952210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.957248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HK2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDKN1C</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.883228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EGR1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TSC22D1      KLF1    MAP2K6  CEBPE  RUNX1T1  MAML2       CBL  \\\n",
       "TSC22D1      0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "KLF1         0.0  0.000000  0.000000    0.0      0.0      0  0.966037   \n",
       "MAP2K6       0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "CEBPE        0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "RUNX1T1      0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "...          ...       ...       ...    ...      ...    ...       ...   \n",
       "CITED1       0.0  0.000000  0.897639    0.0      0.0      0  0.000000   \n",
       "PRDM1        0.0  0.983938  0.952210    0.0      0.0      0  0.957248   \n",
       "HK2          0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "CDKN1C       0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "EGR1         0.0  0.000000  0.000000    0.0      0.0      0  0.000000   \n",
       "\n",
       "            PTPN9  TGFBR2  ETS2  ...      ATL1  NIT1  CDKN1B  PTPN13  HOXA13  \\\n",
       "TSC22D1  0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "KLF1     0.966848     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "MAP2K6   0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "CEBPE    0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "RUNX1T1  0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "...           ...     ...   ...  ...       ...   ...     ...     ...     ...   \n",
       "CITED1   0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "PRDM1    0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "HK2      0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "CDKN1C   0.000000     0.0   0.0  ...  0.883228   0.0     0.0     0.0     0.0   \n",
       "EGR1     0.000000     0.0   0.0  ...  0.000000   0.0     0.0     0.0     0.0   \n",
       "\n",
       "           CITED1     PRDM1  HK2  CDKN1C  EGR1  \n",
       "TSC22D1  0.000000  0.000000    0     0.0   0.0  \n",
       "KLF1     0.000000  0.969909    0     0.0   0.0  \n",
       "MAP2K6   0.893898  0.953321    0     0.0   0.0  \n",
       "CEBPE    0.000000  0.000000    0     0.0   0.0  \n",
       "RUNX1T1  0.000000  0.000000    0     0.0   0.0  \n",
       "...           ...       ...  ...     ...   ...  \n",
       "CITED1   0.000000  0.000000    0     0.0   0.0  \n",
       "PRDM1    0.000000  0.000000    0     0.0   0.0  \n",
       "HK2      0.000000  0.000000    0     0.0   0.0  \n",
       "CDKN1C   0.000000  0.000000    0     0.0   0.0  \n",
       "EGR1     0.000000  0.000000    0     0.0   0.0  \n",
       "\n",
       "[105 rows x 105 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_mtx.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33192ede-f448-4c5a-be5f-0a4f08b1f604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
